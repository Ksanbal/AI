{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #003_DL_1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - 요청이 오면 응답을 받고 실행하는 것\n",
    " - Epochs이 한번 실행될 떄마다 같이 실행된다.\n",
    " \n",
    " \n",
    " 1. 학습이 끝나고 나면 메일을 보내거나 파일로 저장하게 할 수 있다.\n",
    " 2. EarlyStopping기법을 이용해 Overfitting되기 전에 멈추게 할 수 있다.\n",
    " 3. List 형태로 입력하기 때문에, 여러 기능을 추가할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.callbacks.EarlyStopping\n",
    "#monitor : 모니터에 들어간 값이 변하는걸 기반으로 멈춘다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train,y_train),(X_test,y_test) = tf.keras.datasets.imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_doc = tf.keras.datasets.imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train의 데이터 문자로 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as\n",
      "of\n",
      "atmosphere\n",
      "from\n",
      "chest\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-a07258418c30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_word_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for i in X_train[0]:\n",
    "    for k,j in tf.keras.datasets.imdb.get_word_index().items():\n",
    "        if j == X_train[0][i]:\n",
    "            print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_dic = {k:j for j,k in imdb_doc.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'as',\n",
       " 'you',\n",
       " 'with',\n",
       " 'out',\n",
       " 'themselves',\n",
       " 'powerful',\n",
       " 'lets',\n",
       " 'loves',\n",
       " 'their',\n",
       " 'becomes',\n",
       " 'reaching',\n",
       " 'had',\n",
       " 'journalist',\n",
       " 'of',\n",
       " 'lot',\n",
       " 'from',\n",
       " 'anyone',\n",
       " 'to',\n",
       " 'have',\n",
       " 'after',\n",
       " 'out',\n",
       " 'atmosphere',\n",
       " 'never',\n",
       " 'more',\n",
       " 'room',\n",
       " 'titillate',\n",
       " 'it',\n",
       " 'so',\n",
       " 'heart',\n",
       " 'shows',\n",
       " 'to',\n",
       " 'years',\n",
       " 'of',\n",
       " 'every',\n",
       " 'never',\n",
       " 'going',\n",
       " 'villaronga',\n",
       " 'help',\n",
       " 'moments',\n",
       " 'or',\n",
       " 'of',\n",
       " 'every',\n",
       " 'chest',\n",
       " 'visual',\n",
       " 'movie',\n",
       " 'except',\n",
       " 'her',\n",
       " 'was',\n",
       " 'several',\n",
       " 'of',\n",
       " 'enough',\n",
       " 'more',\n",
       " 'with',\n",
       " 'is',\n",
       " 'now',\n",
       " 'current',\n",
       " 'film',\n",
       " 'as',\n",
       " 'you',\n",
       " 'of',\n",
       " 'mine',\n",
       " 'potentially',\n",
       " 'unfortunately',\n",
       " 'of',\n",
       " 'you',\n",
       " 'than',\n",
       " 'him',\n",
       " 'that',\n",
       " 'with',\n",
       " 'out',\n",
       " 'themselves',\n",
       " 'her',\n",
       " 'get',\n",
       " 'for',\n",
       " 'was',\n",
       " 'camp',\n",
       " 'of',\n",
       " 'you',\n",
       " 'movie',\n",
       " 'sometimes',\n",
       " 'movie',\n",
       " 'that',\n",
       " 'with',\n",
       " 'scary',\n",
       " 'but',\n",
       " 'pratfalls',\n",
       " 'to',\n",
       " 'story',\n",
       " 'wonderful',\n",
       " 'that',\n",
       " 'in',\n",
       " 'seeing',\n",
       " 'in',\n",
       " 'character',\n",
       " 'to',\n",
       " 'of',\n",
       " '70s',\n",
       " 'musicians',\n",
       " 'with',\n",
       " 'heart',\n",
       " 'had',\n",
       " 'shadows',\n",
       " 'they',\n",
       " 'of',\n",
       " 'here',\n",
       " 'that',\n",
       " 'with',\n",
       " 'her',\n",
       " 'serious',\n",
       " 'to',\n",
       " 'have',\n",
       " 'does',\n",
       " 'when',\n",
       " 'from',\n",
       " 'why',\n",
       " 'what',\n",
       " 'have',\n",
       " 'critics',\n",
       " 'they',\n",
       " 'is',\n",
       " 'you',\n",
       " 'that',\n",
       " \"isn't\",\n",
       " 'one',\n",
       " 'will',\n",
       " 'very',\n",
       " 'to',\n",
       " 'as',\n",
       " 'itself',\n",
       " 'with',\n",
       " 'other',\n",
       " 'tricky',\n",
       " 'in',\n",
       " 'of',\n",
       " 'seen',\n",
       " 'over',\n",
       " 'landed',\n",
       " 'for',\n",
       " 'anyone',\n",
       " 'of',\n",
       " \"gilmore's\",\n",
       " 'br',\n",
       " \"show's\",\n",
       " 'to',\n",
       " 'whether',\n",
       " 'from',\n",
       " 'than',\n",
       " 'out',\n",
       " 'themselves',\n",
       " 'history',\n",
       " 'he',\n",
       " 'name',\n",
       " 'half',\n",
       " 'some',\n",
       " 'br',\n",
       " 'of',\n",
       " \"'n\",\n",
       " 'odd',\n",
       " 'was',\n",
       " 'two',\n",
       " 'most',\n",
       " 'of',\n",
       " 'mean',\n",
       " 'for',\n",
       " '1',\n",
       " 'any',\n",
       " 'an',\n",
       " 'boat',\n",
       " 'she',\n",
       " 'he',\n",
       " 'should',\n",
       " 'is',\n",
       " 'thought',\n",
       " 'frog',\n",
       " 'but',\n",
       " 'of',\n",
       " 'script',\n",
       " 'you',\n",
       " 'not',\n",
       " 'while',\n",
       " 'history',\n",
       " 'he',\n",
       " 'heart',\n",
       " 'to',\n",
       " 'real',\n",
       " 'at',\n",
       " 'barrel',\n",
       " 'but',\n",
       " 'when',\n",
       " 'from',\n",
       " 'one',\n",
       " 'bit',\n",
       " 'then',\n",
       " 'have',\n",
       " 'two',\n",
       " 'of',\n",
       " 'script',\n",
       " 'their',\n",
       " 'with',\n",
       " 'her',\n",
       " 'nobody',\n",
       " 'most',\n",
       " 'that',\n",
       " 'with',\n",
       " \"wasn't\",\n",
       " 'to',\n",
       " 'with',\n",
       " 'armed',\n",
       " 'acting',\n",
       " 'watch',\n",
       " 'an',\n",
       " 'for',\n",
       " 'with',\n",
       " 'heartfelt',\n",
       " 'film',\n",
       " 'want',\n",
       " 'an']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr = [inverse_dic[i] for i in X_train[0]]\n",
    "tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "방지법 \n",
    " - 모델의 규모를 축소\n",
    " - dropout : 한번 학습할 떄마다 랜덤하게 레이어의 노드를 비활성화해서 학습양을 줄인다.\n",
    " - 수학적으로 증명된 부분이 없기 때문에 여러가지 다른 구조를 사용해 실험을 해봐야만 한다.\n",
    " - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "NUM_WORDS = 10000\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=NUM_WORDS)\n",
    "\n",
    "def multi_hot_sequences(sequences, dimension):\n",
    "    # 0으로 채워진 (len(sequences), dimension) 크기의 행렬을 만듭니다\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, word_indices in enumerate(sequences):\n",
    "        results[i, word_indices] = 1.0  # results[i]의 특정 인덱스만 1로 설정합니다\n",
    "    return results\n",
    "\n",
    "\n",
    "X_train = multi_hot_sequences(X_train, dimension=NUM_WORDS)\n",
    "X_test = multi_hot_sequences(X_test, dimension=NUM_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_54 (Dense)             (None, 16)                160016    \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,305\n",
      "Trainable params: 160,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "baseline_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation='relu', input_shape=(10000,)),\n",
    "    #input_shape나 input layer가 없으면 layer구축이 되지않는다.\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.4), #윗줄의 layer에 dropout을 적용한다.(overfitting을 막아주지만, 학습이 안될 수 있다.)\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "baseline_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['acc','binary_crossentropy']\n",
    ")\n",
    "\n",
    "baseline_model.summary() #model의 layer가 어떻게 구성됬는지 알려준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 2s 76us/sample - loss: 0.3518 - acc: 0.8515 - binary_crossentropy: 0.3518\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 2s 65us/sample - loss: 0.2061 - acc: 0.9238 - binary_crossentropy: 0.2061\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 2s 66us/sample - loss: 0.1474 - acc: 0.9469 - binary_crossentropy: 0.1474\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 2s 69us/sample - loss: 0.1057 - acc: 0.9624 - binary_crossentropy: 0.1057\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 2s 78us/sample - loss: 0.0756 - acc: 0.9731 - binary_crossentropy: 0.0756\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 2s 78us/sample - loss: 0.0533 - acc: 0.9812 - binary_crossentropy: 0.0533\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 2s 77us/sample - loss: 0.0386 - acc: 0.9872 - binary_crossentropy: 0.03861s - loss: 0.0309 - acc: 0.9898 - binar\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 2s 67us/sample - loss: 0.0313 - acc: 0.9898 - binary_crossentropy: 0.0313\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 2s 67us/sample - loss: 0.0212 - acc: 0.9936 - binary_crossentropy: 0.0212\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 2s 67us/sample - loss: 0.0235 - acc: 0.9926 - binary_crossentropy: 0.0235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb47a77860>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_model.fit(X_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 2s 83us/sample - loss: 0.9278 - acc: 0.8590 - binary_crossentropy: 0.9278\n",
      "25000/25000 [==============================] - 1s 41us/sample - loss: 0.9278 - acc: 0.8590 - binary_crossentropy: 0.9278\n",
      "25000/25000 [==============================] - 1s 37us/sample - loss: 0.9278 - acc: 0.8590 - binary_crossentropy: 0.9278\n",
      "25000/25000 [==============================] - 1s 37us/sample - loss: 0.9278 - acc: 0.8590 - binary_crossentropy: 0.9278\n",
      "25000/25000 [==============================] - 1s 37us/sample - loss: 0.9278 - acc: 0.8590 - binary_crossentropy: 0.9278\n"
     ]
    }
   ],
   "source": [
    "#노드 16\n",
    "for i in range(5):\n",
    "    baseline_model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 1s 51us/sample - loss: 0.9916 - acc: 0.8671 - binary_crossentropy: 0.9916\n",
      "25000/25000 [==============================] - 1s 48us/sample - loss: 0.9916 - acc: 0.8671 - binary_crossentropy: 0.9916\n",
      "25000/25000 [==============================] - 1s 48us/sample - loss: 0.9916 - acc: 0.8671 - binary_crossentropy: 0.9916\n",
      "25000/25000 [==============================] - 1s 49us/sample - loss: 0.9916 - acc: 0.8671 - binary_crossentropy: 0.99161s - loss: 1.0532 - acc: 0.8607 - bin\n",
      "25000/25000 [==============================] - 1s 49us/sample - loss: 0.9916 - acc: 0.8671 - binary_crossentropy: 0.9916\n"
     ]
    }
   ],
   "source": [
    "#노드 64\n",
    "for i in range(5):\n",
    "    baseline_model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 2s 84us/sample - loss: 0.9844 - acc: 0.8522 - binary_crossentropy: 0.9844\n",
      "25000/25000 [==============================] - 1s 38us/sample - loss: 0.9844 - acc: 0.8522 - binary_crossentropy: 0.98440s - loss: 1.0029 - acc: 0.8496 - binary_crossentropy: \n",
      "25000/25000 [==============================] - 1s 36us/sample - loss: 0.9844 - acc: 0.8522 - binary_crossentropy: 0.9844\n",
      "25000/25000 [==============================] - 1s 37us/sample - loss: 0.9844 - acc: 0.8522 - binary_crossentropy: 0.9844\n",
      "25000/25000 [==============================] - 1s 37us/sample - loss: 0.9844 - acc: 0.8522 - binary_crossentropy: 0.9844\n"
     ]
    }
   ],
   "source": [
    "#노드 16 + dropout(0.4)\n",
    "for i in range(5):\n",
    "    baseline_model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.keras.callbacks.History"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(baseline_model.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'History' object has no attribute 'histroy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-134-0feee0b41413>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbaseline_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistroy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'History' object has no attribute 'histroy'"
     ]
    }
   ],
   "source": [
    "for i,j in baseline_model.history:\n",
    "    print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'validation_data': None,\n",
       " 'model': <tensorflow.python.keras.engine.sequential.Sequential at 0xb44934630>,\n",
       " '_chief_worker_only': None,\n",
       " 'params': {'batch_size': 32,\n",
       "  'epochs': 10,\n",
       "  'steps': None,\n",
       "  'samples': 25000,\n",
       "  'verbose': 1,\n",
       "  'do_validation': False,\n",
       "  'metrics': ['loss', 'acc', 'binary_crossentropy']},\n",
       " 'epoch': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       " 'history': {'loss': [0.3518049849128723,\n",
       "   0.20605560847759247,\n",
       "   0.1474023206627369,\n",
       "   0.10565813039720058,\n",
       "   0.07562767586946488,\n",
       "   0.053289363467283545,\n",
       "   0.03863822119017597,\n",
       "   0.03126803087219596,\n",
       "   0.021178269228171558,\n",
       "   0.02350306481948297],\n",
       "  'acc': [0.85152,\n",
       "   0.92376,\n",
       "   0.94692,\n",
       "   0.96236,\n",
       "   0.97312,\n",
       "   0.98124,\n",
       "   0.98716,\n",
       "   0.98976,\n",
       "   0.99356,\n",
       "   0.99264],\n",
       "  'binary_crossentropy': [0.3518053,\n",
       "   0.20605558,\n",
       "   0.14740242,\n",
       "   0.10565812,\n",
       "   0.075627685,\n",
       "   0.053289384,\n",
       "   0.03863822,\n",
       "   0.031268034,\n",
       "   0.02117827,\n",
       "   0.023503063]}}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(baseline_model.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Restore model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - yaml : 마크업언어, 설정파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train,y_train),(X_test,y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_target = y_train[:1000]\n",
    "test_target = y_test[:1000]\n",
    "\n",
    "train_data,test_data = X_train[:1000].reshape(-1,28*28)/255.0, X_test[:1000].reshape(-1,28*28)/255.0\n",
    "#2차원 데이터를 flatten한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_63 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(512, activation='relu', input_shape=(784,)),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "        metrics=['acc']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ModelCheckpoint : callback에서 한번 할때마다 저장된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_callback = tf.keras.callbacks.ModelCheckpoint('training_1/cp.ckpt',\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples\n",
      "Epoch 1/10\n",
      " 768/1000 [======================>.......] - ETA: 0s - loss: 0.0282 - acc: 1.0000\n",
      "Epoch 00001: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 0s 183us/sample - loss: 0.0284 - acc: 1.0000\n",
      "Epoch 2/10\n",
      " 576/1000 [================>.............] - ETA: 0s - loss: 0.0206 - acc: 1.0000\n",
      "Epoch 00002: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 0s 124us/sample - loss: 0.0221 - acc: 1.0000\n",
      "Epoch 3/10\n",
      " 576/1000 [================>.............] - ETA: 0s - loss: 0.0200 - acc: 1.0000\n",
      "Epoch 00003: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 0s 119us/sample - loss: 0.0203 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " 576/1000 [================>.............] - ETA: 0s - loss: 0.0158 - acc: 1.0000\n",
      "Epoch 00004: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 0s 117us/sample - loss: 0.0160 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " 384/1000 [==========>...................] - ETA: 0s - loss: 0.0155 - acc: 1.0000\n",
      "Epoch 00005: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 0s 130us/sample - loss: 0.0148 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " 576/1000 [================>.............] - ETA: 0s - loss: 0.0129 - acc: 1.0000\n",
      "Epoch 00006: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 0s 111us/sample - loss: 0.0120 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " 608/1000 [=================>............] - ETA: 0s - loss: 0.0118 - acc: 1.0000\n",
      "Epoch 00007: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 0s 115us/sample - loss: 0.0123 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " 704/1000 [====================>.........] - ETA: 0s - loss: 0.0105 - acc: 1.0000\n",
      "Epoch 00008: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 0s 114us/sample - loss: 0.0103 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " 576/1000 [================>.............] - ETA: 0s - loss: 0.0096 - acc: 1.0000\n",
      "Epoch 00009: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 0s 115us/sample - loss: 0.0096 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " 576/1000 [================>.............] - ETA: 0s - loss: 0.0073 - acc: 1.0000\n",
      "Epoch 00010: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 0s 112us/sample - loss: 0.0079 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb56180da0>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, train_target,  epochs = 10, callbacks = [cp_callback]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0xb474e1898>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('training_1/cp.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - 매 에포크마다 다 다른파일로 저장하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# 파일 이름에 에포크 번호를 포함시킵니다(`str.format` 포맷)\n",
    "checkpoint_path = \"training_2/cp-{epoch:04d}.ckpt\" # 매 에포크마다 다 다른 파일로 저장할 수 있다.\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path, verbose=1, save_weights_only=True,\n",
    "    # 다섯 번째 에포크마다 가중치를 저장합니다\n",
    "    period=5)\n",
    "\n",
    "model = create_model()\n",
    "model.save_weights(checkpoint_path.format(epoch=0))\n",
    "model.fit(train_images, train_labels,\n",
    "          epochs = 50, callbacks = [cp_callback],\n",
    "          validation_data = (test_images,test_labels),\n",
    "          verbose=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - 가장 마지막의 에포크를 저장하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "latest\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
