{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #003_DL_1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - 요청이 오면 응답을 받고 실행하는 것\n",
    " - Epochs이 한번 실행될 떄마다 같이 실행된다.\n",
    " \n",
    " \n",
    " 1. 학습이 끝나고 나면 메일을 보내거나 파일로 저장하게 할 수 있다.\n",
    " 2. EarlyStopping기법을 이용해 Overfitting되기 전에 멈추게 할 수 있다.\n",
    " 3. List 형태로 입력하기 때문에, 여러 기능을 추가할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.callbacks.EarlyStopping(monitor= , patience= )\n",
    "#monitor : 모니터에 들어간 값이 변하는걸 기반으로 멈춘다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train,y_train),(X_test,y_test) = tf.keras.datasets.imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fawn': 34701,\n",
       " 'tsukino': 52006,\n",
       " 'nunnery': 52007,\n",
       " 'sonja': 16816,\n",
       " 'vani': 63951,\n",
       " 'woods': 1408,\n",
       " 'spiders': 16115,\n",
       " 'hanging': 2345,\n",
       " 'woody': 2289,\n",
       " 'trawling': 52008,\n",
       " \"hold's\": 52009,\n",
       " 'comically': 11307,\n",
       " 'localized': 40830,\n",
       " 'disobeying': 30568,\n",
       " \"'royale\": 52010,\n",
       " \"harpo's\": 40831,\n",
       " 'canet': 52011,\n",
       " 'aileen': 19313,\n",
       " 'acurately': 52012,\n",
       " \"diplomat's\": 52013,\n",
       " 'rickman': 25242,\n",
       " 'arranged': 6746,\n",
       " 'rumbustious': 52014,\n",
       " 'familiarness': 52015,\n",
       " \"spider'\": 52016,\n",
       " 'hahahah': 68804,\n",
       " \"wood'\": 52017,\n",
       " 'transvestism': 40833,\n",
       " \"hangin'\": 34702,\n",
       " 'bringing': 2338,\n",
       " 'seamier': 40834,\n",
       " 'wooded': 34703,\n",
       " 'bravora': 52018,\n",
       " 'grueling': 16817,\n",
       " 'wooden': 1636,\n",
       " 'wednesday': 16818,\n",
       " \"'prix\": 52019,\n",
       " 'altagracia': 34704,\n",
       " 'circuitry': 52020,\n",
       " 'crotch': 11585,\n",
       " 'busybody': 57766,\n",
       " \"tart'n'tangy\": 52021,\n",
       " 'burgade': 14129,\n",
       " 'thrace': 52023,\n",
       " \"tom's\": 11038,\n",
       " 'snuggles': 52025,\n",
       " 'francesco': 29114,\n",
       " 'complainers': 52027,\n",
       " 'templarios': 52125,\n",
       " '272': 40835,\n",
       " '273': 52028,\n",
       " 'zaniacs': 52130,\n",
       " '275': 34706,\n",
       " 'consenting': 27631,\n",
       " 'snuggled': 40836,\n",
       " 'inanimate': 15492,\n",
       " 'uality': 52030,\n",
       " 'bronte': 11926,\n",
       " 'errors': 4010,\n",
       " 'dialogs': 3230,\n",
       " \"yomada's\": 52031,\n",
       " \"madman's\": 34707,\n",
       " 'dialoge': 30585,\n",
       " 'usenet': 52033,\n",
       " 'videodrome': 40837,\n",
       " \"kid'\": 26338,\n",
       " 'pawed': 52034,\n",
       " \"'girlfriend'\": 30569,\n",
       " \"'pleasure\": 52035,\n",
       " \"'reloaded'\": 52036,\n",
       " \"kazakos'\": 40839,\n",
       " 'rocque': 52037,\n",
       " 'mailings': 52038,\n",
       " 'brainwashed': 11927,\n",
       " 'mcanally': 16819,\n",
       " \"tom''\": 52039,\n",
       " 'kurupt': 25243,\n",
       " 'affiliated': 21905,\n",
       " 'babaganoosh': 52040,\n",
       " \"noe's\": 40840,\n",
       " 'quart': 40841,\n",
       " 'kids': 359,\n",
       " 'uplifting': 5034,\n",
       " 'controversy': 7093,\n",
       " 'kida': 21906,\n",
       " 'kidd': 23379,\n",
       " \"error'\": 52041,\n",
       " 'neurologist': 52042,\n",
       " 'spotty': 18510,\n",
       " 'cobblers': 30570,\n",
       " 'projection': 9878,\n",
       " 'fastforwarding': 40842,\n",
       " 'sters': 52043,\n",
       " \"eggar's\": 52044,\n",
       " 'etherything': 52045,\n",
       " 'gateshead': 40843,\n",
       " 'airball': 34708,\n",
       " 'unsinkable': 25244,\n",
       " 'stern': 7180,\n",
       " \"cervi's\": 52046,\n",
       " 'dnd': 40844,\n",
       " 'dna': 11586,\n",
       " 'insecurity': 20598,\n",
       " \"'reboot'\": 52047,\n",
       " 'trelkovsky': 11037,\n",
       " 'jaekel': 52048,\n",
       " 'sidebars': 52049,\n",
       " \"sforza's\": 52050,\n",
       " 'distortions': 17633,\n",
       " 'mutinies': 52051,\n",
       " 'sermons': 30602,\n",
       " '7ft': 40846,\n",
       " 'boobage': 52052,\n",
       " \"o'bannon's\": 52053,\n",
       " 'populations': 23380,\n",
       " 'chulak': 52054,\n",
       " 'mesmerize': 27633,\n",
       " 'quinnell': 52055,\n",
       " 'yahoo': 10307,\n",
       " 'meteorologist': 52057,\n",
       " 'beswick': 42577,\n",
       " 'boorman': 15493,\n",
       " 'voicework': 40847,\n",
       " \"ster'\": 52058,\n",
       " 'blustering': 22922,\n",
       " 'hj': 52059,\n",
       " 'intake': 27634,\n",
       " 'morally': 5621,\n",
       " 'jumbling': 40849,\n",
       " 'bowersock': 52060,\n",
       " \"'porky's'\": 52061,\n",
       " 'gershon': 16821,\n",
       " 'ludicrosity': 40850,\n",
       " 'coprophilia': 52062,\n",
       " 'expressively': 40851,\n",
       " \"india's\": 19500,\n",
       " \"post's\": 34710,\n",
       " 'wana': 52063,\n",
       " 'wang': 5283,\n",
       " 'wand': 30571,\n",
       " 'wane': 25245,\n",
       " 'edgeways': 52321,\n",
       " 'titanium': 34711,\n",
       " 'pinta': 40852,\n",
       " 'want': 178,\n",
       " 'pinto': 30572,\n",
       " 'whoopdedoodles': 52065,\n",
       " 'tchaikovsky': 21908,\n",
       " 'travel': 2103,\n",
       " \"'victory'\": 52066,\n",
       " 'copious': 11928,\n",
       " 'gouge': 22433,\n",
       " \"chapters'\": 52067,\n",
       " 'barbra': 6702,\n",
       " 'uselessness': 30573,\n",
       " \"wan'\": 52068,\n",
       " 'assimilated': 27635,\n",
       " 'petiot': 16116,\n",
       " 'most\\x85and': 52069,\n",
       " 'dinosaurs': 3930,\n",
       " 'wrong': 352,\n",
       " 'seda': 52070,\n",
       " 'stollen': 52071,\n",
       " 'sentencing': 34712,\n",
       " 'ouroboros': 40853,\n",
       " 'assimilates': 40854,\n",
       " 'colorfully': 40855,\n",
       " 'glenne': 27636,\n",
       " 'dongen': 52072,\n",
       " 'subplots': 4760,\n",
       " 'kiloton': 52073,\n",
       " 'chandon': 23381,\n",
       " \"effect'\": 34713,\n",
       " 'snugly': 27637,\n",
       " 'kuei': 40856,\n",
       " 'welcomed': 9092,\n",
       " 'dishonor': 30071,\n",
       " 'concurrence': 52075,\n",
       " 'stoicism': 23382,\n",
       " \"guys'\": 14896,\n",
       " \"beroemd'\": 52077,\n",
       " 'butcher': 6703,\n",
       " \"melfi's\": 40857,\n",
       " 'aargh': 30623,\n",
       " 'playhouse': 20599,\n",
       " 'wickedly': 11308,\n",
       " 'fit': 1180,\n",
       " 'labratory': 52078,\n",
       " 'lifeline': 40859,\n",
       " 'screaming': 1927,\n",
       " 'fix': 4287,\n",
       " 'cineliterate': 52079,\n",
       " 'fic': 52080,\n",
       " 'fia': 52081,\n",
       " 'fig': 34714,\n",
       " 'fmvs': 52082,\n",
       " 'fie': 52083,\n",
       " 'reentered': 52084,\n",
       " 'fin': 30574,\n",
       " 'doctresses': 52085,\n",
       " 'fil': 52086,\n",
       " 'zucker': 12606,\n",
       " 'ached': 31931,\n",
       " 'counsil': 52088,\n",
       " 'paterfamilias': 52089,\n",
       " 'songwriter': 13885,\n",
       " 'shivam': 34715,\n",
       " 'hurting': 9654,\n",
       " 'effects': 299,\n",
       " 'slauther': 52090,\n",
       " \"'flame'\": 52091,\n",
       " 'sommerset': 52092,\n",
       " 'interwhined': 52093,\n",
       " 'whacking': 27638,\n",
       " 'bartok': 52094,\n",
       " 'barton': 8775,\n",
       " 'frewer': 21909,\n",
       " \"fi'\": 52095,\n",
       " 'ingrid': 6192,\n",
       " 'stribor': 30575,\n",
       " 'approporiately': 52096,\n",
       " 'wobblyhand': 52097,\n",
       " 'tantalisingly': 52098,\n",
       " 'ankylosaurus': 52099,\n",
       " 'parasites': 17634,\n",
       " 'childen': 52100,\n",
       " \"jenkins'\": 52101,\n",
       " 'metafiction': 52102,\n",
       " 'golem': 17635,\n",
       " 'indiscretion': 40860,\n",
       " \"reeves'\": 23383,\n",
       " \"inamorata's\": 57781,\n",
       " 'brittannica': 52104,\n",
       " 'adapt': 7916,\n",
       " \"russo's\": 30576,\n",
       " 'guitarists': 48246,\n",
       " 'abbott': 10553,\n",
       " 'abbots': 40861,\n",
       " 'lanisha': 17649,\n",
       " 'magickal': 40863,\n",
       " 'mattter': 52105,\n",
       " \"'willy\": 52106,\n",
       " 'pumpkins': 34716,\n",
       " 'stuntpeople': 52107,\n",
       " 'estimate': 30577,\n",
       " 'ugghhh': 40864,\n",
       " 'gameplay': 11309,\n",
       " \"wern't\": 52108,\n",
       " \"n'sync\": 40865,\n",
       " 'sickeningly': 16117,\n",
       " 'chiara': 40866,\n",
       " 'disturbed': 4011,\n",
       " 'portmanteau': 40867,\n",
       " 'ineffectively': 52109,\n",
       " \"duchonvey's\": 82143,\n",
       " \"nasty'\": 37519,\n",
       " 'purpose': 1285,\n",
       " 'lazers': 52112,\n",
       " 'lightened': 28105,\n",
       " 'kaliganj': 52113,\n",
       " 'popularism': 52114,\n",
       " \"damme's\": 18511,\n",
       " 'stylistics': 30578,\n",
       " 'mindgaming': 52115,\n",
       " 'spoilerish': 46449,\n",
       " \"'corny'\": 52117,\n",
       " 'boerner': 34718,\n",
       " 'olds': 6792,\n",
       " 'bakelite': 52118,\n",
       " 'renovated': 27639,\n",
       " 'forrester': 27640,\n",
       " \"lumiere's\": 52119,\n",
       " 'gaskets': 52024,\n",
       " 'needed': 884,\n",
       " 'smight': 34719,\n",
       " 'master': 1297,\n",
       " \"edie's\": 25905,\n",
       " 'seeber': 40868,\n",
       " 'hiya': 52120,\n",
       " 'fuzziness': 52121,\n",
       " 'genesis': 14897,\n",
       " 'rewards': 12607,\n",
       " 'enthrall': 30579,\n",
       " \"'about\": 40869,\n",
       " \"recollection's\": 52122,\n",
       " 'mutilated': 11039,\n",
       " 'fatherlands': 52123,\n",
       " \"fischer's\": 52124,\n",
       " 'positively': 5399,\n",
       " '270': 34705,\n",
       " 'ahmed': 34720,\n",
       " 'zatoichi': 9836,\n",
       " 'bannister': 13886,\n",
       " 'anniversaries': 52127,\n",
       " \"helm's\": 30580,\n",
       " \"'work'\": 52128,\n",
       " 'exclaimed': 34721,\n",
       " \"'unfunny'\": 52129,\n",
       " '274': 52029,\n",
       " 'feeling': 544,\n",
       " \"wanda's\": 52131,\n",
       " 'dolan': 33266,\n",
       " '278': 52133,\n",
       " 'peacoat': 52134,\n",
       " 'brawny': 40870,\n",
       " 'mishra': 40871,\n",
       " 'worlders': 40872,\n",
       " 'protags': 52135,\n",
       " 'skullcap': 52136,\n",
       " 'dastagir': 57596,\n",
       " 'affairs': 5622,\n",
       " 'wholesome': 7799,\n",
       " 'hymen': 52137,\n",
       " 'paramedics': 25246,\n",
       " 'unpersons': 52138,\n",
       " 'heavyarms': 52139,\n",
       " 'affaire': 52140,\n",
       " 'coulisses': 52141,\n",
       " 'hymer': 40873,\n",
       " 'kremlin': 52142,\n",
       " 'shipments': 30581,\n",
       " 'pixilated': 52143,\n",
       " \"'00s\": 30582,\n",
       " 'diminishing': 18512,\n",
       " 'cinematic': 1357,\n",
       " 'resonates': 14898,\n",
       " 'simplify': 40874,\n",
       " \"nature'\": 40875,\n",
       " 'temptresses': 40876,\n",
       " 'reverence': 16822,\n",
       " 'resonated': 19502,\n",
       " 'dailey': 34722,\n",
       " '2\\x85': 52144,\n",
       " 'treize': 27641,\n",
       " 'majo': 52145,\n",
       " 'kiya': 21910,\n",
       " 'woolnough': 52146,\n",
       " 'thanatos': 39797,\n",
       " 'sandoval': 35731,\n",
       " 'dorama': 40879,\n",
       " \"o'shaughnessy\": 52147,\n",
       " 'tech': 4988,\n",
       " 'fugitives': 32018,\n",
       " 'teck': 30583,\n",
       " \"'e'\": 76125,\n",
       " 'doesn’t': 40881,\n",
       " 'purged': 52149,\n",
       " 'saying': 657,\n",
       " \"martians'\": 41095,\n",
       " 'norliss': 23418,\n",
       " 'dickey': 27642,\n",
       " 'dicker': 52152,\n",
       " \"'sependipity\": 52153,\n",
       " 'padded': 8422,\n",
       " 'ordell': 57792,\n",
       " \"sturges'\": 40882,\n",
       " 'independentcritics': 52154,\n",
       " 'tempted': 5745,\n",
       " \"atkinson's\": 34724,\n",
       " 'hounded': 25247,\n",
       " 'apace': 52155,\n",
       " 'clicked': 15494,\n",
       " \"'humor'\": 30584,\n",
       " \"martino's\": 17177,\n",
       " \"'supporting\": 52156,\n",
       " 'warmongering': 52032,\n",
       " \"zemeckis's\": 34725,\n",
       " 'lube': 21911,\n",
       " 'shocky': 52157,\n",
       " 'plate': 7476,\n",
       " 'plata': 40883,\n",
       " 'sturgess': 40884,\n",
       " \"nerds'\": 40885,\n",
       " 'plato': 20600,\n",
       " 'plath': 34726,\n",
       " 'platt': 40886,\n",
       " 'mcnab': 52159,\n",
       " 'clumsiness': 27643,\n",
       " 'altogether': 3899,\n",
       " 'massacring': 42584,\n",
       " 'bicenntinial': 52160,\n",
       " 'skaal': 40887,\n",
       " 'droning': 14360,\n",
       " 'lds': 8776,\n",
       " 'jaguar': 21912,\n",
       " \"cale's\": 34727,\n",
       " 'nicely': 1777,\n",
       " 'mummy': 4588,\n",
       " \"lot's\": 18513,\n",
       " 'patch': 10086,\n",
       " 'kerkhof': 50202,\n",
       " \"leader's\": 52161,\n",
       " \"'movie\": 27644,\n",
       " 'uncomfirmed': 52162,\n",
       " 'heirloom': 40888,\n",
       " 'wrangle': 47360,\n",
       " 'emotion\\x85': 52163,\n",
       " \"'stargate'\": 52164,\n",
       " 'pinoy': 40889,\n",
       " 'conchatta': 40890,\n",
       " 'broeke': 41128,\n",
       " 'advisedly': 40891,\n",
       " \"barker's\": 17636,\n",
       " 'descours': 52166,\n",
       " 'lots': 772,\n",
       " 'lotr': 9259,\n",
       " 'irs': 9879,\n",
       " 'lott': 52167,\n",
       " 'xvi': 40892,\n",
       " 'irk': 34728,\n",
       " 'irl': 52168,\n",
       " 'ira': 6887,\n",
       " 'belzer': 21913,\n",
       " 'irc': 52169,\n",
       " 'ire': 27645,\n",
       " 'requisites': 40893,\n",
       " 'discipline': 7693,\n",
       " 'lyoko': 52961,\n",
       " 'extend': 11310,\n",
       " 'nature': 873,\n",
       " \"'dickie'\": 52170,\n",
       " 'optimist': 40894,\n",
       " 'lapping': 30586,\n",
       " 'superficial': 3900,\n",
       " 'vestment': 52171,\n",
       " 'extent': 2823,\n",
       " 'tendons': 52172,\n",
       " \"heller's\": 52173,\n",
       " 'quagmires': 52174,\n",
       " 'miyako': 52175,\n",
       " 'moocow': 20601,\n",
       " \"coles'\": 52176,\n",
       " 'lookit': 40895,\n",
       " 'ravenously': 52177,\n",
       " 'levitating': 40896,\n",
       " 'perfunctorily': 52178,\n",
       " 'lookin': 30587,\n",
       " \"lot'\": 40898,\n",
       " 'lookie': 52179,\n",
       " 'fearlessly': 34870,\n",
       " 'libyan': 52181,\n",
       " 'fondles': 40899,\n",
       " 'gopher': 35714,\n",
       " 'wearying': 40901,\n",
       " \"nz's\": 52182,\n",
       " 'minuses': 27646,\n",
       " 'puposelessly': 52183,\n",
       " 'shandling': 52184,\n",
       " 'decapitates': 31268,\n",
       " 'humming': 11929,\n",
       " \"'nother\": 40902,\n",
       " 'smackdown': 21914,\n",
       " 'underdone': 30588,\n",
       " 'frf': 40903,\n",
       " 'triviality': 52185,\n",
       " 'fro': 25248,\n",
       " 'bothers': 8777,\n",
       " \"'kensington\": 52186,\n",
       " 'much': 73,\n",
       " 'muco': 34730,\n",
       " 'wiseguy': 22615,\n",
       " \"richie's\": 27648,\n",
       " 'tonino': 40904,\n",
       " 'unleavened': 52187,\n",
       " 'fry': 11587,\n",
       " \"'tv'\": 40905,\n",
       " 'toning': 40906,\n",
       " 'obese': 14361,\n",
       " 'sensationalized': 30589,\n",
       " 'spiv': 40907,\n",
       " 'spit': 6259,\n",
       " 'arkin': 7364,\n",
       " 'charleton': 21915,\n",
       " 'jeon': 16823,\n",
       " 'boardroom': 21916,\n",
       " 'doubts': 4989,\n",
       " 'spin': 3084,\n",
       " 'hepo': 53083,\n",
       " 'wildcat': 27649,\n",
       " 'venoms': 10584,\n",
       " 'misconstrues': 52191,\n",
       " 'mesmerising': 18514,\n",
       " 'misconstrued': 40908,\n",
       " 'rescinds': 52192,\n",
       " 'prostrate': 52193,\n",
       " 'majid': 40909,\n",
       " 'climbed': 16479,\n",
       " 'canoeing': 34731,\n",
       " 'majin': 52195,\n",
       " 'animie': 57804,\n",
       " 'sylke': 40910,\n",
       " 'conditioned': 14899,\n",
       " 'waddell': 40911,\n",
       " '3\\x85': 52196,\n",
       " 'hyperdrive': 41188,\n",
       " 'conditioner': 34732,\n",
       " 'bricklayer': 53153,\n",
       " 'hong': 2576,\n",
       " 'memoriam': 52198,\n",
       " 'inventively': 30592,\n",
       " \"levant's\": 25249,\n",
       " 'portobello': 20638,\n",
       " 'remand': 52200,\n",
       " 'mummified': 19504,\n",
       " 'honk': 27650,\n",
       " 'spews': 19505,\n",
       " 'visitations': 40912,\n",
       " 'mummifies': 52201,\n",
       " 'cavanaugh': 25250,\n",
       " 'zeon': 23385,\n",
       " \"jungle's\": 40913,\n",
       " 'viertel': 34733,\n",
       " 'frenchmen': 27651,\n",
       " 'torpedoes': 52202,\n",
       " 'schlessinger': 52203,\n",
       " 'torpedoed': 34734,\n",
       " 'blister': 69876,\n",
       " 'cinefest': 52204,\n",
       " 'furlough': 34735,\n",
       " 'mainsequence': 52205,\n",
       " 'mentors': 40914,\n",
       " 'academic': 9094,\n",
       " 'stillness': 20602,\n",
       " 'academia': 40915,\n",
       " 'lonelier': 52206,\n",
       " 'nibby': 52207,\n",
       " \"losers'\": 52208,\n",
       " 'cineastes': 40916,\n",
       " 'corporate': 4449,\n",
       " 'massaging': 40917,\n",
       " 'bellow': 30593,\n",
       " 'absurdities': 19506,\n",
       " 'expetations': 53241,\n",
       " 'nyfiken': 40918,\n",
       " 'mehras': 75638,\n",
       " 'lasse': 52209,\n",
       " 'visability': 52210,\n",
       " 'militarily': 33946,\n",
       " \"elder'\": 52211,\n",
       " 'gainsbourg': 19023,\n",
       " 'hah': 20603,\n",
       " 'hai': 13420,\n",
       " 'haj': 34736,\n",
       " 'hak': 25251,\n",
       " 'hal': 4311,\n",
       " 'ham': 4892,\n",
       " 'duffer': 53259,\n",
       " 'haa': 52213,\n",
       " 'had': 66,\n",
       " 'advancement': 11930,\n",
       " 'hag': 16825,\n",
       " \"hand'\": 25252,\n",
       " 'hay': 13421,\n",
       " 'mcnamara': 20604,\n",
       " \"mozart's\": 52214,\n",
       " 'duffel': 30731,\n",
       " 'haq': 30594,\n",
       " 'har': 13887,\n",
       " 'has': 44,\n",
       " 'hat': 2401,\n",
       " 'hav': 40919,\n",
       " 'haw': 30595,\n",
       " 'figtings': 52215,\n",
       " 'elders': 15495,\n",
       " 'underpanted': 52216,\n",
       " 'pninson': 52217,\n",
       " 'unequivocally': 27652,\n",
       " \"barbara's\": 23673,\n",
       " \"bello'\": 52219,\n",
       " 'indicative': 12997,\n",
       " 'yawnfest': 40920,\n",
       " 'hexploitation': 52220,\n",
       " \"loder's\": 52221,\n",
       " 'sleuthing': 27653,\n",
       " \"justin's\": 32622,\n",
       " \"'ball\": 52222,\n",
       " \"'summer\": 52223,\n",
       " \"'demons'\": 34935,\n",
       " \"mormon's\": 52225,\n",
       " \"laughton's\": 34737,\n",
       " 'debell': 52226,\n",
       " 'shipyard': 39724,\n",
       " 'unabashedly': 30597,\n",
       " 'disks': 40401,\n",
       " 'crowd': 2290,\n",
       " 'crowe': 10087,\n",
       " \"vancouver's\": 56434,\n",
       " 'mosques': 34738,\n",
       " 'crown': 6627,\n",
       " 'culpas': 52227,\n",
       " 'crows': 27654,\n",
       " 'surrell': 53344,\n",
       " 'flowless': 52229,\n",
       " 'sheirk': 52230,\n",
       " \"'three\": 40923,\n",
       " \"peterson'\": 52231,\n",
       " 'ooverall': 52232,\n",
       " 'perchance': 40924,\n",
       " 'bottom': 1321,\n",
       " 'chabert': 53363,\n",
       " 'sneha': 52233,\n",
       " 'inhuman': 13888,\n",
       " 'ichii': 52234,\n",
       " 'ursla': 52235,\n",
       " 'completly': 30598,\n",
       " 'moviedom': 40925,\n",
       " 'raddick': 52236,\n",
       " 'brundage': 51995,\n",
       " 'brigades': 40926,\n",
       " 'starring': 1181,\n",
       " \"'goal'\": 52237,\n",
       " 'caskets': 52238,\n",
       " 'willcock': 52239,\n",
       " \"threesome's\": 52240,\n",
       " \"mosque'\": 52241,\n",
       " \"cover's\": 52242,\n",
       " 'spaceships': 17637,\n",
       " 'anomalous': 40927,\n",
       " 'ptsd': 27655,\n",
       " 'shirdan': 52243,\n",
       " 'obscenity': 21962,\n",
       " 'lemmings': 30599,\n",
       " 'duccio': 30600,\n",
       " \"levene's\": 52244,\n",
       " \"'gorby'\": 52245,\n",
       " \"teenager's\": 25255,\n",
       " 'marshall': 5340,\n",
       " 'honeymoon': 9095,\n",
       " 'shoots': 3231,\n",
       " 'despised': 12258,\n",
       " 'okabasho': 52246,\n",
       " 'fabric': 8289,\n",
       " 'cannavale': 18515,\n",
       " 'raped': 3537,\n",
       " \"tutt's\": 52247,\n",
       " 'grasping': 17638,\n",
       " 'despises': 18516,\n",
       " \"thief's\": 40928,\n",
       " 'rapes': 8926,\n",
       " 'raper': 52248,\n",
       " \"eyre'\": 27656,\n",
       " 'walchek': 52249,\n",
       " \"elmo's\": 23386,\n",
       " 'perfumes': 40929,\n",
       " 'spurting': 21918,\n",
       " \"exposition'\\x85\": 52250,\n",
       " 'denoting': 52251,\n",
       " 'thesaurus': 34740,\n",
       " \"shoot'\": 40930,\n",
       " 'bonejack': 49759,\n",
       " 'simpsonian': 52253,\n",
       " 'hebetude': 30601,\n",
       " \"hallow's\": 34741,\n",
       " 'desperation\\x85': 52254,\n",
       " 'incinerator': 34742,\n",
       " 'congratulations': 10308,\n",
       " 'humbled': 52255,\n",
       " \"else's\": 5924,\n",
       " 'trelkovski': 40845,\n",
       " \"rape'\": 52256,\n",
       " \"'chapters'\": 59386,\n",
       " '1600s': 52257,\n",
       " 'martian': 7253,\n",
       " 'nicest': 25256,\n",
       " 'eyred': 52259,\n",
       " 'passenger': 9457,\n",
       " 'disgrace': 6041,\n",
       " 'moderne': 52260,\n",
       " 'barrymore': 5120,\n",
       " 'yankovich': 52261,\n",
       " 'moderns': 40931,\n",
       " 'studliest': 52262,\n",
       " 'bedsheet': 52263,\n",
       " 'decapitation': 14900,\n",
       " 'slurring': 52264,\n",
       " \"'nunsploitation'\": 52265,\n",
       " \"'character'\": 34743,\n",
       " 'cambodia': 9880,\n",
       " 'rebelious': 52266,\n",
       " 'pasadena': 27657,\n",
       " 'crowne': 40932,\n",
       " \"'bedchamber\": 52267,\n",
       " 'conjectural': 52268,\n",
       " 'appologize': 52269,\n",
       " 'halfassing': 52270,\n",
       " 'paycheque': 57816,\n",
       " 'palms': 20606,\n",
       " \"'islands\": 52271,\n",
       " 'hawked': 40933,\n",
       " 'palme': 21919,\n",
       " 'conservatively': 40934,\n",
       " 'larp': 64007,\n",
       " 'palma': 5558,\n",
       " 'smelling': 21920,\n",
       " 'aragorn': 12998,\n",
       " 'hawker': 52272,\n",
       " 'hawkes': 52273,\n",
       " 'explosions': 3975,\n",
       " 'loren': 8059,\n",
       " \"pyle's\": 52274,\n",
       " 'shootout': 6704,\n",
       " \"mike's\": 18517,\n",
       " \"driscoll's\": 52275,\n",
       " 'cogsworth': 40935,\n",
       " \"britian's\": 52276,\n",
       " 'childs': 34744,\n",
       " \"portrait's\": 52277,\n",
       " 'chain': 3626,\n",
       " 'whoever': 2497,\n",
       " 'puttered': 52278,\n",
       " 'childe': 52279,\n",
       " 'maywether': 52280,\n",
       " 'chair': 3036,\n",
       " \"rance's\": 52281,\n",
       " 'machu': 34745,\n",
       " 'ballet': 4517,\n",
       " 'grapples': 34746,\n",
       " 'summerize': 76152,\n",
       " 'freelance': 30603,\n",
       " \"andrea's\": 52283,\n",
       " '\\x91very': 52284,\n",
       " 'coolidge': 45879,\n",
       " 'mache': 18518,\n",
       " 'balled': 52285,\n",
       " 'grappled': 40937,\n",
       " 'macha': 18519,\n",
       " 'underlining': 21921,\n",
       " 'macho': 5623,\n",
       " 'oversight': 19507,\n",
       " 'machi': 25257,\n",
       " 'verbally': 11311,\n",
       " 'tenacious': 21922,\n",
       " 'windshields': 40938,\n",
       " 'paychecks': 18557,\n",
       " 'jerk': 3396,\n",
       " \"good'\": 11931,\n",
       " 'prancer': 34748,\n",
       " 'prances': 21923,\n",
       " 'olympus': 52286,\n",
       " 'lark': 21924,\n",
       " 'embark': 10785,\n",
       " 'gloomy': 7365,\n",
       " 'jehaan': 52287,\n",
       " 'turaqui': 52288,\n",
       " \"child'\": 20607,\n",
       " 'locked': 2894,\n",
       " 'pranced': 52289,\n",
       " 'exact': 2588,\n",
       " 'unattuned': 52290,\n",
       " 'minute': 783,\n",
       " 'skewed': 16118,\n",
       " 'hodgins': 40940,\n",
       " 'skewer': 34749,\n",
       " 'think\\x85': 52291,\n",
       " 'rosenstein': 38765,\n",
       " 'helmit': 52292,\n",
       " 'wrestlemanias': 34750,\n",
       " 'hindered': 16826,\n",
       " \"martha's\": 30604,\n",
       " 'cheree': 52293,\n",
       " \"pluckin'\": 52294,\n",
       " 'ogles': 40941,\n",
       " 'heavyweight': 11932,\n",
       " 'aada': 82190,\n",
       " 'chopping': 11312,\n",
       " 'strongboy': 61534,\n",
       " 'hegemonic': 41342,\n",
       " 'adorns': 40942,\n",
       " 'xxth': 41346,\n",
       " 'nobuhiro': 34751,\n",
       " 'capitães': 52298,\n",
       " 'kavogianni': 52299,\n",
       " 'antwerp': 13422,\n",
       " 'celebrated': 6538,\n",
       " 'roarke': 52300,\n",
       " 'baggins': 40943,\n",
       " 'cheeseburgers': 31270,\n",
       " 'matras': 52301,\n",
       " \"nineties'\": 52302,\n",
       " \"'craig'\": 52303,\n",
       " 'celebrates': 12999,\n",
       " 'unintentionally': 3383,\n",
       " 'drafted': 14362,\n",
       " 'climby': 52304,\n",
       " '303': 52305,\n",
       " 'oldies': 18520,\n",
       " 'climbs': 9096,\n",
       " 'honour': 9655,\n",
       " 'plucking': 34752,\n",
       " '305': 30074,\n",
       " 'address': 5514,\n",
       " 'menjou': 40944,\n",
       " \"'freak'\": 42592,\n",
       " 'dwindling': 19508,\n",
       " 'benson': 9458,\n",
       " 'white’s': 52307,\n",
       " 'shamelessness': 40945,\n",
       " 'impacted': 21925,\n",
       " 'upatz': 52308,\n",
       " 'cusack': 3840,\n",
       " \"flavia's\": 37567,\n",
       " 'effette': 52309,\n",
       " 'influx': 34753,\n",
       " 'boooooooo': 52310,\n",
       " 'dimitrova': 52311,\n",
       " 'houseman': 13423,\n",
       " 'bigas': 25259,\n",
       " 'boylen': 52312,\n",
       " 'phillipenes': 52313,\n",
       " 'fakery': 40946,\n",
       " \"grandpa's\": 27658,\n",
       " 'darnell': 27659,\n",
       " 'undergone': 19509,\n",
       " 'handbags': 52315,\n",
       " 'perished': 21926,\n",
       " 'pooped': 37778,\n",
       " 'vigour': 27660,\n",
       " 'opposed': 3627,\n",
       " 'etude': 52316,\n",
       " \"caine's\": 11799,\n",
       " 'doozers': 52317,\n",
       " 'photojournals': 34754,\n",
       " 'perishes': 52318,\n",
       " 'constrains': 34755,\n",
       " 'migenes': 40948,\n",
       " 'consoled': 30605,\n",
       " 'alastair': 16827,\n",
       " 'wvs': 52319,\n",
       " 'ooooooh': 52320,\n",
       " 'approving': 34756,\n",
       " 'consoles': 40949,\n",
       " 'disparagement': 52064,\n",
       " 'futureistic': 52322,\n",
       " 'rebounding': 52323,\n",
       " \"'date\": 52324,\n",
       " 'gregoire': 52325,\n",
       " 'rutherford': 21927,\n",
       " 'americanised': 34757,\n",
       " 'novikov': 82196,\n",
       " 'following': 1042,\n",
       " 'munroe': 34758,\n",
       " \"morita'\": 52326,\n",
       " 'christenssen': 52327,\n",
       " 'oatmeal': 23106,\n",
       " 'fossey': 25260,\n",
       " 'livered': 40950,\n",
       " 'listens': 13000,\n",
       " \"'marci\": 76164,\n",
       " \"otis's\": 52330,\n",
       " 'thanking': 23387,\n",
       " 'maude': 16019,\n",
       " 'extensions': 34759,\n",
       " 'ameteurish': 52332,\n",
       " \"commender's\": 52333,\n",
       " 'agricultural': 27661,\n",
       " 'convincingly': 4518,\n",
       " 'fueled': 17639,\n",
       " 'mahattan': 54014,\n",
       " \"paris's\": 40952,\n",
       " 'vulkan': 52336,\n",
       " 'stapes': 52337,\n",
       " 'odysessy': 52338,\n",
       " 'harmon': 12259,\n",
       " 'surfing': 4252,\n",
       " 'halloran': 23494,\n",
       " 'unbelieveably': 49580,\n",
       " \"'offed'\": 52339,\n",
       " 'quadrant': 30607,\n",
       " 'inhabiting': 19510,\n",
       " 'nebbish': 34760,\n",
       " 'forebears': 40953,\n",
       " 'skirmish': 34761,\n",
       " 'ocassionally': 52340,\n",
       " \"'resist\": 52341,\n",
       " 'impactful': 21928,\n",
       " 'spicier': 52342,\n",
       " 'touristy': 40954,\n",
       " \"'football'\": 52343,\n",
       " 'webpage': 40955,\n",
       " 'exurbia': 52345,\n",
       " 'jucier': 52346,\n",
       " 'professors': 14901,\n",
       " 'structuring': 34762,\n",
       " 'jig': 30608,\n",
       " 'overlord': 40956,\n",
       " 'disconnect': 25261,\n",
       " 'sniffle': 82201,\n",
       " 'slimeball': 40957,\n",
       " 'jia': 40958,\n",
       " 'milked': 16828,\n",
       " 'banjoes': 40959,\n",
       " 'jim': 1237,\n",
       " 'workforces': 52348,\n",
       " 'jip': 52349,\n",
       " 'rotweiller': 52350,\n",
       " 'mundaneness': 34763,\n",
       " \"'ninja'\": 52351,\n",
       " \"dead'\": 11040,\n",
       " \"cipriani's\": 40960,\n",
       " 'modestly': 20608,\n",
       " \"professor'\": 52352,\n",
       " 'shacked': 40961,\n",
       " 'bashful': 34764,\n",
       " 'sorter': 23388,\n",
       " 'overpowering': 16120,\n",
       " 'workmanlike': 18521,\n",
       " 'henpecked': 27662,\n",
       " 'sorted': 18522,\n",
       " \"jōb's\": 52354,\n",
       " \"'always\": 52355,\n",
       " \"'baptists\": 34765,\n",
       " 'dreamcatchers': 52356,\n",
       " \"'silence'\": 52357,\n",
       " 'hickory': 21929,\n",
       " 'fun\\x97yet': 52358,\n",
       " 'breakumentary': 52359,\n",
       " 'didn': 15496,\n",
       " 'didi': 52360,\n",
       " 'pealing': 52361,\n",
       " 'dispite': 40962,\n",
       " \"italy's\": 25262,\n",
       " 'instability': 21930,\n",
       " 'quarter': 6539,\n",
       " 'quartet': 12608,\n",
       " 'padmé': 52362,\n",
       " \"'bleedmedry\": 52363,\n",
       " 'pahalniuk': 52364,\n",
       " 'honduras': 52365,\n",
       " 'bursting': 10786,\n",
       " \"pablo's\": 41465,\n",
       " 'irremediably': 52367,\n",
       " 'presages': 40963,\n",
       " 'bowlegged': 57832,\n",
       " 'dalip': 65183,\n",
       " 'entering': 6260,\n",
       " 'newsradio': 76172,\n",
       " 'presaged': 54150,\n",
       " \"giallo's\": 27663,\n",
       " 'bouyant': 40964,\n",
       " 'amerterish': 52368,\n",
       " 'rajni': 18523,\n",
       " 'leeves': 30610,\n",
       " 'macauley': 34767,\n",
       " 'seriously': 612,\n",
       " 'sugercoma': 52369,\n",
       " 'grimstead': 52370,\n",
       " \"'fairy'\": 52371,\n",
       " 'zenda': 30611,\n",
       " \"'twins'\": 52372,\n",
       " 'realisation': 17640,\n",
       " 'highsmith': 27664,\n",
       " 'raunchy': 7817,\n",
       " 'incentives': 40965,\n",
       " 'flatson': 52374,\n",
       " 'snooker': 35097,\n",
       " 'crazies': 16829,\n",
       " 'crazier': 14902,\n",
       " 'grandma': 7094,\n",
       " 'napunsaktha': 52375,\n",
       " 'workmanship': 30612,\n",
       " 'reisner': 52376,\n",
       " \"sanford's\": 61306,\n",
       " '\\x91doña': 52377,\n",
       " 'modest': 6108,\n",
       " \"everything's\": 19153,\n",
       " 'hamer': 40966,\n",
       " \"couldn't'\": 52379,\n",
       " 'quibble': 13001,\n",
       " 'socking': 52380,\n",
       " 'tingler': 21931,\n",
       " 'gutman': 52381,\n",
       " 'lachlan': 40967,\n",
       " 'tableaus': 52382,\n",
       " 'headbanger': 52383,\n",
       " 'spoken': 2847,\n",
       " 'cerebrally': 34768,\n",
       " \"'road\": 23490,\n",
       " 'tableaux': 21932,\n",
       " \"proust's\": 40968,\n",
       " 'periodical': 40969,\n",
       " \"shoveller's\": 52385,\n",
       " 'tamara': 25263,\n",
       " 'affords': 17641,\n",
       " 'concert': 3249,\n",
       " \"yara's\": 87955,\n",
       " 'someome': 52386,\n",
       " 'lingering': 8424,\n",
       " \"abraham's\": 41511,\n",
       " 'beesley': 34769,\n",
       " 'cherbourg': 34770,\n",
       " 'kagan': 28624,\n",
       " 'snatch': 9097,\n",
       " \"miyazaki's\": 9260,\n",
       " 'absorbs': 25264,\n",
       " \"koltai's\": 40970,\n",
       " 'tingled': 64027,\n",
       " 'crossroads': 19511,\n",
       " 'rehab': 16121,\n",
       " 'falworth': 52389,\n",
       " 'sequals': 52390,\n",
       " ...}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_doc = tf.keras.datasets.imdb.get_word_index()\n",
    "imdb_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train의 데이터 문자로 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 22665,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 21631,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 19193,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 5244,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 10311,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 5952,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 31050,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 12118,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 7486,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 5535,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 5345,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert redford's is an amazing actor and now the same being director norman's father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the part's of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all "
     ]
    }
   ],
   "source": [
    "for i in X_train[0]:\n",
    "    for j,k in imdb_doc.items():\n",
    "        if k == i-3:\n",
    "            print(j, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_dic = {k:j for j,k in imdb_doc.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'as',\n",
       " 'you',\n",
       " 'with',\n",
       " 'out',\n",
       " 'themselves',\n",
       " 'powerful',\n",
       " 'lets',\n",
       " 'loves',\n",
       " 'their',\n",
       " 'becomes',\n",
       " 'reaching',\n",
       " 'had',\n",
       " 'journalist',\n",
       " 'of',\n",
       " 'lot',\n",
       " 'from',\n",
       " 'anyone',\n",
       " 'to',\n",
       " 'have',\n",
       " 'after',\n",
       " 'out',\n",
       " 'atmosphere',\n",
       " 'never',\n",
       " 'more',\n",
       " 'room',\n",
       " 'titillate',\n",
       " 'it',\n",
       " 'so',\n",
       " 'heart',\n",
       " 'shows',\n",
       " 'to',\n",
       " 'years',\n",
       " 'of',\n",
       " 'every',\n",
       " 'never',\n",
       " 'going',\n",
       " 'villaronga',\n",
       " 'help',\n",
       " 'moments',\n",
       " 'or',\n",
       " 'of',\n",
       " 'every',\n",
       " 'chest',\n",
       " 'visual',\n",
       " 'movie',\n",
       " 'except',\n",
       " 'her',\n",
       " 'was',\n",
       " 'several',\n",
       " 'of',\n",
       " 'enough',\n",
       " 'more',\n",
       " 'with',\n",
       " 'is',\n",
       " 'now',\n",
       " 'current',\n",
       " 'film',\n",
       " 'as',\n",
       " 'you',\n",
       " 'of',\n",
       " 'mine',\n",
       " 'potentially',\n",
       " 'unfortunately',\n",
       " 'of',\n",
       " 'you',\n",
       " 'than',\n",
       " 'him',\n",
       " 'that',\n",
       " 'with',\n",
       " 'out',\n",
       " 'themselves',\n",
       " 'her',\n",
       " 'get',\n",
       " 'for',\n",
       " 'was',\n",
       " 'camp',\n",
       " 'of',\n",
       " 'you',\n",
       " 'movie',\n",
       " 'sometimes',\n",
       " 'movie',\n",
       " 'that',\n",
       " 'with',\n",
       " 'scary',\n",
       " 'but',\n",
       " 'pratfalls',\n",
       " 'to',\n",
       " 'story',\n",
       " 'wonderful',\n",
       " 'that',\n",
       " 'in',\n",
       " 'seeing',\n",
       " 'in',\n",
       " 'character',\n",
       " 'to',\n",
       " 'of',\n",
       " '70s',\n",
       " 'musicians',\n",
       " 'with',\n",
       " 'heart',\n",
       " 'had',\n",
       " 'shadows',\n",
       " 'they',\n",
       " 'of',\n",
       " 'here',\n",
       " 'that',\n",
       " 'with',\n",
       " 'her',\n",
       " 'serious',\n",
       " 'to',\n",
       " 'have',\n",
       " 'does',\n",
       " 'when',\n",
       " 'from',\n",
       " 'why',\n",
       " 'what',\n",
       " 'have',\n",
       " 'critics',\n",
       " 'they',\n",
       " 'is',\n",
       " 'you',\n",
       " 'that',\n",
       " \"isn't\",\n",
       " 'one',\n",
       " 'will',\n",
       " 'very',\n",
       " 'to',\n",
       " 'as',\n",
       " 'itself',\n",
       " 'with',\n",
       " 'other',\n",
       " 'tricky',\n",
       " 'in',\n",
       " 'of',\n",
       " 'seen',\n",
       " 'over',\n",
       " 'landed',\n",
       " 'for',\n",
       " 'anyone',\n",
       " 'of',\n",
       " \"gilmore's\",\n",
       " 'br',\n",
       " \"show's\",\n",
       " 'to',\n",
       " 'whether',\n",
       " 'from',\n",
       " 'than',\n",
       " 'out',\n",
       " 'themselves',\n",
       " 'history',\n",
       " 'he',\n",
       " 'name',\n",
       " 'half',\n",
       " 'some',\n",
       " 'br',\n",
       " 'of',\n",
       " \"'n\",\n",
       " 'odd',\n",
       " 'was',\n",
       " 'two',\n",
       " 'most',\n",
       " 'of',\n",
       " 'mean',\n",
       " 'for',\n",
       " '1',\n",
       " 'any',\n",
       " 'an',\n",
       " 'boat',\n",
       " 'she',\n",
       " 'he',\n",
       " 'should',\n",
       " 'is',\n",
       " 'thought',\n",
       " 'frog',\n",
       " 'but',\n",
       " 'of',\n",
       " 'script',\n",
       " 'you',\n",
       " 'not',\n",
       " 'while',\n",
       " 'history',\n",
       " 'he',\n",
       " 'heart',\n",
       " 'to',\n",
       " 'real',\n",
       " 'at',\n",
       " 'barrel',\n",
       " 'but',\n",
       " 'when',\n",
       " 'from',\n",
       " 'one',\n",
       " 'bit',\n",
       " 'then',\n",
       " 'have',\n",
       " 'two',\n",
       " 'of',\n",
       " 'script',\n",
       " 'their',\n",
       " 'with',\n",
       " 'her',\n",
       " 'nobody',\n",
       " 'most',\n",
       " 'that',\n",
       " 'with',\n",
       " \"wasn't\",\n",
       " 'to',\n",
       " 'with',\n",
       " 'armed',\n",
       " 'acting',\n",
       " 'watch',\n",
       " 'an',\n",
       " 'for',\n",
       " 'with',\n",
       " 'heartfelt',\n",
       " 'film',\n",
       " 'want',\n",
       " 'an']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr = [inverse_dic[i] for i in X_train[0]]\n",
    "tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "방지법 \n",
    " - 모델의 규모를 축소\n",
    " - dropout : 한번 학습할 떄마다 랜덤하게 레이어의 노드를 비활성화해서 학습양을 줄인다.\n",
    " - 수학적으로 증명된 부분이 없기 때문에 여러가지 다른 구조를 사용해 실험을 해봐야만 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "NUM_WORDS = 10000\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=NUM_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 10000)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#멀티 인코딩\n",
    "def multi_hot_sequences(sequences, dimension):\n",
    "    # 0으로 채워진 (len(sequences), dimension) 크기의 행렬을 만듭니다\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, word_indices in enumerate(sequences):\n",
    "        results[i, word_indices] = 1.0  # results[i]의 특정 인덱스만 1로 설정합니다\n",
    "    return results\n",
    "\n",
    "\n",
    "X_train = multi_hot_sequences(X_train, dimension=NUM_WORDS)\n",
    "X_test = multi_hot_sequences(X_test, dimension=NUM_WORDS)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 88585)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "test = mlb.fit_transform(X_train)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 16)                160016    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,305\n",
      "Trainable params: 160,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "baseline_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation='relu', input_shape=(10000,)),\n",
    "    #input_shape나 input layer가 없으면 layer구축이 되지않는다.\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.4), #윗줄의 layer에 dropout을 적용한다.(overfitting을 막아주지만, 학습이 안될 수 있다.)\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "baseline_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['acc','binary_crossentropy']\n",
    ")\n",
    "\n",
    "baseline_model.summary() #model의 layer가 어떻게 구성됬는지 알려준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 2s 69us/sample - loss: 0.0163 - acc: 0.9956 - binary_crossentropy: 0.01630s - loss: 0.0140 - acc: 0.9964 - binary_crosse\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 2s 65us/sample - loss: 0.0099 - acc: 0.9972 - binary_crossentropy: 0.0099\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 2s 66us/sample - loss: 0.0100 - acc: 0.9971 - binary_crossentropy: 0.0100\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 2s 66us/sample - loss: 0.0138 - acc: 0.9959 - binary_crossentropy: 0.0138\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 2s 66us/sample - loss: 0.0125 - acc: 0.9968 - binary_crossentropy: 0.0125\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 2s 66us/sample - loss: 0.0113 - acc: 0.9971 - binary_crossentropy: 0.0113\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 2s 68us/sample - loss: 0.0071 - acc: 0.9986 - binary_crossentropy: 0.0071\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 2s 67us/sample - loss: 0.0075 - acc: 0.9981 - binary_crossentropy: 0.0075\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 2s 67us/sample - loss: 0.0051 - acc: 0.9986 - binary_crossentropy: 0.0051\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 2s 67us/sample - loss: 0.0052 - acc: 0.9986 - binary_crossentropy: 0.0052\n"
     ]
    }
   ],
   "source": [
    "basehistory = baseline_model.fit(X_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 2s 83us/sample - loss: 0.9278 - acc: 0.8590 - binary_crossentropy: 0.9278\n",
      "25000/25000 [==============================] - 1s 41us/sample - loss: 0.9278 - acc: 0.8590 - binary_crossentropy: 0.9278\n",
      "25000/25000 [==============================] - 1s 37us/sample - loss: 0.9278 - acc: 0.8590 - binary_crossentropy: 0.9278\n",
      "25000/25000 [==============================] - 1s 37us/sample - loss: 0.9278 - acc: 0.8590 - binary_crossentropy: 0.9278\n",
      "25000/25000 [==============================] - 1s 37us/sample - loss: 0.9278 - acc: 0.8590 - binary_crossentropy: 0.9278\n"
     ]
    }
   ],
   "source": [
    "#노드 16\n",
    "for i in range(5):\n",
    "    baseline_model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 1s 51us/sample - loss: 0.9916 - acc: 0.8671 - binary_crossentropy: 0.9916\n",
      "25000/25000 [==============================] - 1s 48us/sample - loss: 0.9916 - acc: 0.8671 - binary_crossentropy: 0.9916\n",
      "25000/25000 [==============================] - 1s 48us/sample - loss: 0.9916 - acc: 0.8671 - binary_crossentropy: 0.9916\n",
      "25000/25000 [==============================] - 1s 49us/sample - loss: 0.9916 - acc: 0.8671 - binary_crossentropy: 0.99161s - loss: 1.0532 - acc: 0.8607 - bin\n",
      "25000/25000 [==============================] - 1s 49us/sample - loss: 0.9916 - acc: 0.8671 - binary_crossentropy: 0.9916\n"
     ]
    }
   ],
   "source": [
    "#노드 64\n",
    "for i in range(5):\n",
    "    baseline_model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 2s 84us/sample - loss: 0.9844 - acc: 0.8522 - binary_crossentropy: 0.9844\n",
      "25000/25000 [==============================] - 1s 38us/sample - loss: 0.9844 - acc: 0.8522 - binary_crossentropy: 0.98440s - loss: 1.0029 - acc: 0.8496 - binary_crossentropy: \n",
      "25000/25000 [==============================] - 1s 36us/sample - loss: 0.9844 - acc: 0.8522 - binary_crossentropy: 0.9844\n",
      "25000/25000 [==============================] - 1s 37us/sample - loss: 0.9844 - acc: 0.8522 - binary_crossentropy: 0.9844\n",
      "25000/25000 [==============================] - 1s 37us/sample - loss: 0.9844 - acc: 0.8522 - binary_crossentropy: 0.9844\n"
     ]
    }
   ],
   "source": [
    "#노드 16 + dropout(0.4)\n",
    "for i in range(5):\n",
    "    baseline_model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(basehistory.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.016333775931065903,\n",
       "  0.009859354893685376,\n",
       "  0.00995865374022862,\n",
       "  0.013814313817459624,\n",
       "  0.012485013283735608,\n",
       "  0.011285245397350955,\n",
       "  0.007091213868167542,\n",
       "  0.007488024125037482,\n",
       "  0.00510383877664357,\n",
       "  0.005200694738331586],\n",
       " 'acc': [0.99564,\n",
       "  0.9972,\n",
       "  0.99708,\n",
       "  0.99588,\n",
       "  0.9968,\n",
       "  0.99712,\n",
       "  0.99856,\n",
       "  0.99812,\n",
       "  0.9986,\n",
       "  0.9986],\n",
       " 'binary_crossentropy': [0.016333772,\n",
       "  0.009859355,\n",
       "  0.0099586565,\n",
       "  0.01381432,\n",
       "  0.01248501,\n",
       "  0.011285237,\n",
       "  0.0070912135,\n",
       "  0.00748802,\n",
       "  0.0051038372,\n",
       "  0.0052006934]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basehistory.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss\n",
      "acc\n",
      "binary_crossentropy\n"
     ]
    }
   ],
   "source": [
    "for i in basehistory.history:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_self_setattr_tracking': True,\n",
       " '_name': 'sequential',\n",
       " '_activity_regularizer': None,\n",
       " '_trainable': True,\n",
       " '_dynamic': False,\n",
       " '_is_compiled': True,\n",
       " '_layers': [<tensorflow.python.keras.engine.input_layer.InputLayer at 0xb2d3b4ac8>,\n",
       "  <tensorflow.python.keras.layers.core.Dense at 0xb2f945438>,\n",
       "  <tensorflow.python.keras.layers.core.Dense at 0xb2f945588>,\n",
       "  <tensorflow.python.keras.layers.core.Dropout at 0xb2f9451d0>,\n",
       "  <tensorflow.python.keras.layers.core.Dense at 0xb2f9450b8>],\n",
       " '_compute_output_and_mask_jointly': True,\n",
       " 'supports_masking': False,\n",
       " 'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam at 0xb2d3adb38>,\n",
       " '_trainable_weights': [],\n",
       " '_non_trainable_weights': [],\n",
       " '_updates': [],\n",
       " '_losses': [],\n",
       " '_eager_losses': [],\n",
       " '_callable_losses': [],\n",
       " '_metrics': [],\n",
       " '_metrics_tensors': {},\n",
       " '_scope': None,\n",
       " '_reuse': None,\n",
       " '_graph': None,\n",
       " '_dtype': None,\n",
       " '_outbound_nodes': [],\n",
       " '_inbound_nodes': [<tensorflow.python.keras.engine.base_layer.Node at 0xb2f935d30>],\n",
       " '_trackable_saver': <tensorflow.python.training.tracking.util.TrackableSaver at 0xb2d386be0>,\n",
       " '_mixed_precision_policy': <tensorflow.python.keras.mixed_precision.experimental.policy.Policy at 0xb2fa0c710>,\n",
       " '_is_graph_network': True,\n",
       " '_expects_training_arg': True,\n",
       " '_expects_mask_arg': True,\n",
       " '_call_convention': <CallConvention.EXPLICIT_INPUTS_ARGUMENT: 1>,\n",
       " 'outputs': [<tf.Tensor 'dense_2/Identity:0' shape=(None, 1) dtype=float32>],\n",
       " 'inputs': [<tf.Tensor 'dense_input:0' shape=(None, 10000) dtype=float32>],\n",
       " 'built': True,\n",
       " '_distribution_strategy': None,\n",
       " '_compile_distribution': False,\n",
       " '_run_eagerly': None,\n",
       " '_build_input_shape': None,\n",
       " '_layer_call_argspecs': {<tensorflow.python.keras.engine.input_layer.InputLayer at 0xb2d3b4ac8>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.core.Dense at 0xb2f945438>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.core.Dense at 0xb2f945588>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.core.Dropout at 0xb2f9451d0>: FullArgSpec(args=['self', 'inputs', 'training'], varargs=None, varkw=None, defaults=(None,), kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.core.Dense at 0xb2f9450b8>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={})},\n",
       " '_nested_outputs': <tf.Tensor 'dense_2/Identity:0' shape=(None, 1) dtype=float32>,\n",
       " '_nested_inputs': <tf.Tensor 'dense_input:0' shape=(None, 10000) dtype=float32>,\n",
       " '_input_layers': [<tensorflow.python.keras.engine.input_layer.InputLayer at 0xb2d3b4ac8>],\n",
       " '_output_layers': [<tensorflow.python.keras.layers.core.Dense at 0xb2f9450b8>],\n",
       " '_input_coordinates': [(<tensorflow.python.keras.engine.input_layer.InputLayer at 0xb2d3b4ac8>,\n",
       "   0,\n",
       "   0)],\n",
       " '_output_coordinates': [(<tensorflow.python.keras.layers.core.Dense at 0xb2f9450b8>,\n",
       "   0,\n",
       "   0)],\n",
       " '_output_mask_cache': {},\n",
       " '_output_tensor_cache': {},\n",
       " '_output_shape_cache': {},\n",
       " '_network_nodes': {'dense_1_ib-0',\n",
       "  'dense_2_ib-0',\n",
       "  'dense_ib-0',\n",
       "  'dense_input_ib-0',\n",
       "  'dropout_ib-0'},\n",
       " '_nodes_by_depth': defaultdict(list,\n",
       "             {0: [<tensorflow.python.keras.engine.base_layer.Node at 0xb38293b00>],\n",
       "              1: [<tensorflow.python.keras.engine.base_layer.Node at 0xb2d3b19b0>],\n",
       "              2: [<tensorflow.python.keras.engine.base_layer.Node at 0xb2d3adbe0>],\n",
       "              3: [<tensorflow.python.keras.engine.base_layer.Node at 0xb2f9c0668>],\n",
       "              4: [<tensorflow.python.keras.engine.base_layer.Node at 0xb2d3ade48>]}),\n",
       " '_layers_by_depth': defaultdict(list,\n",
       "             {0: [<tensorflow.python.keras.layers.core.Dense at 0xb2f9450b8>],\n",
       "              1: [<tensorflow.python.keras.layers.core.Dropout at 0xb2f9451d0>],\n",
       "              2: [<tensorflow.python.keras.layers.core.Dense at 0xb2f945588>],\n",
       "              3: [<tensorflow.python.keras.layers.core.Dense at 0xb2f945438>],\n",
       "              4: [<tensorflow.python.keras.engine.input_layer.InputLayer at 0xb2d3b4ac8>]}),\n",
       " '_self_unconditional_checkpoint_dependencies': [TrackableReference(name='layer-0', ref=<tensorflow.python.keras.engine.input_layer.InputLayer object at 0xb2d3b4ac8>),\n",
       "  TrackableReference(name='layer_with_weights-0', ref=<tensorflow.python.keras.layers.core.Dense object at 0xb2f945438>),\n",
       "  TrackableReference(name='layer-1', ref=<tensorflow.python.keras.layers.core.Dense object at 0xb2f945438>),\n",
       "  TrackableReference(name='layer_with_weights-1', ref=<tensorflow.python.keras.layers.core.Dense object at 0xb2f945588>),\n",
       "  TrackableReference(name='layer-2', ref=<tensorflow.python.keras.layers.core.Dense object at 0xb2f945588>),\n",
       "  TrackableReference(name='layer-3', ref=<tensorflow.python.keras.layers.core.Dropout object at 0xb2f9451d0>),\n",
       "  TrackableReference(name='layer_with_weights-2', ref=<tensorflow.python.keras.layers.core.Dense object at 0xb2f9450b8>),\n",
       "  TrackableReference(name='layer-4', ref=<tensorflow.python.keras.layers.core.Dense object at 0xb2f9450b8>),\n",
       "  TrackableReference(name='optimizer', ref=<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0xb2d3adb38>)],\n",
       " '_self_unconditional_dependency_names': {'layer-0': <tensorflow.python.keras.engine.input_layer.InputLayer at 0xb2d3b4ac8>,\n",
       "  'layer_with_weights-0': <tensorflow.python.keras.layers.core.Dense at 0xb2f945438>,\n",
       "  'layer-1': <tensorflow.python.keras.layers.core.Dense at 0xb2f945438>,\n",
       "  'layer_with_weights-1': <tensorflow.python.keras.layers.core.Dense at 0xb2f945588>,\n",
       "  'layer-2': <tensorflow.python.keras.layers.core.Dense at 0xb2f945588>,\n",
       "  'layer-3': <tensorflow.python.keras.layers.core.Dropout at 0xb2f9451d0>,\n",
       "  'layer_with_weights-2': <tensorflow.python.keras.layers.core.Dense at 0xb2f9450b8>,\n",
       "  'layer-4': <tensorflow.python.keras.layers.core.Dense at 0xb2f9450b8>,\n",
       "  'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam at 0xb2d3adb38>},\n",
       " '_self_unconditional_deferred_dependencies': {},\n",
       " '_self_update_uid': -1,\n",
       " '_self_name_based_restores': set(),\n",
       " 'output_names': ['dense_2'],\n",
       " 'input_names': ['dense_input'],\n",
       " '_feed_input_names': ['dense_input'],\n",
       " '_feed_inputs': [<tf.Tensor 'dense_input:0' shape=(None, 10000) dtype=float32>],\n",
       " '_feed_input_shapes': [(None, 10000)],\n",
       " '_cloning': False,\n",
       " 'loss': 'binary_crossentropy',\n",
       " 'loss_weights': None,\n",
       " 'sample_weight_mode': None,\n",
       " '_compile_metrics': ['acc', 'binary_crossentropy'],\n",
       " '_compile_weighted_metrics': None,\n",
       " '_training_endpoints': [<tensorflow.python.keras.engine.training._TrainingEndpoint at 0xb2d3adc18>],\n",
       " '_distributed_model_cache': {},\n",
       " '_distributed_function_cache': {},\n",
       " '_compile_metrics_names': ['loss', 'acc', 'binary_crossentropy'],\n",
       " '_compile_metric_functions': [<tensorflow.python.keras.metrics.MeanMetricWrapper at 0xb4a0b8470>,\n",
       "  <tensorflow.python.keras.metrics.MeanMetricWrapper at 0xb411de828>],\n",
       " '_compile_metrics_tensors': {'acc': <tf.Tensor 'metrics/acc/Identity:0' shape=() dtype=float32>,\n",
       "  'binary_crossentropy': <tf.Tensor 'metrics/binary_crossentropy/Identity:0' shape=() dtype=float32>},\n",
       " 'loss_functions': [<tensorflow.python.keras.losses.LossFunctionWrapper at 0xb2d3ada20>],\n",
       " '_per_output_metrics': [OrderedDict([('acc',\n",
       "                <tensorflow.python.keras.metrics.MeanMetricWrapper at 0xb4a0b8470>),\n",
       "               ('binary_crossentropy',\n",
       "                <tensorflow.python.keras.metrics.MeanMetricWrapper at 0xb411de828>)])],\n",
       " '_per_output_weighted_metrics': [OrderedDict()],\n",
       " 'total_loss': <tf.Tensor 'loss/mul:0' shape=() dtype=float32>,\n",
       " '_function_kwargs': {},\n",
       " 'train_function': <tensorflow.python.keras.backend.EagerExecutionFunction at 0xb2fa0c0f0>,\n",
       " 'test_function': None,\n",
       " 'predict_function': None,\n",
       " '_collected_trainable_weights': [<tf.Variable 'dense/kernel:0' shape=(10000, 16) dtype=float32, numpy=\n",
       "  array([[ 0.00780175,  0.00165286,  0.00578181, ...,  0.01707032,\n",
       "           0.01279455, -0.01380039],\n",
       "         [-0.03537124,  0.04471422, -0.02668973, ...,  0.06743989,\n",
       "          -0.00931145,  0.00284343],\n",
       "         [-0.03985853,  0.04636933, -0.05125376, ...,  0.04369435,\n",
       "          -0.0025329 , -0.00492737],\n",
       "         ...,\n",
       "         [ 0.02221088, -0.21348004,  0.03312377, ...,  0.04577366,\n",
       "           0.03195994,  0.09717522],\n",
       "         [-0.00159376,  0.12328047,  0.03617099, ..., -0.03678872,\n",
       "          -0.09821745, -0.12028424],\n",
       "         [ 0.04007591,  0.09725554,  0.08027516, ..., -0.01077283,\n",
       "          -0.03459255, -0.01495849]], dtype=float32)>,\n",
       "  <tf.Variable 'dense/bias:0' shape=(16,) dtype=float32, numpy=\n",
       "  array([-0.04904266,  0.04556651, -0.03326995, -0.01497537,  0.1192585 ,\n",
       "         -0.05864103,  0.00458875, -0.06893186,  0.02337879, -0.02629858,\n",
       "          0.05778653,  0.1180435 , -0.02692159,  0.06910858,  0.00199265,\n",
       "         -0.01311643], dtype=float32)>,\n",
       "  <tf.Variable 'dense_1/kernel:0' shape=(16, 16) dtype=float32, numpy=\n",
       "  array([[-0.5540732 ,  0.5743823 , -0.25179014, -0.3996789 , -0.53866506,\n",
       "           0.49626493, -0.42347887,  0.20661467, -0.4856292 ,  0.6762683 ,\n",
       "          -0.70541084,  0.46042356,  0.3782949 , -0.53386337, -0.27155817,\n",
       "           0.33582214],\n",
       "         [-0.71709245,  0.8139371 , -0.7824374 , -0.60740304, -0.52024394,\n",
       "           0.82747936, -0.5703225 ,  0.5096528 , -0.48965117,  1.1791676 ,\n",
       "          -0.9024884 ,  0.95850277,  0.70832705, -0.66904473, -0.6867456 ,\n",
       "           0.6514571 ],\n",
       "         [ 0.4352152 , -0.35965234,  0.31435642,  0.33201396, -0.2376774 ,\n",
       "          -0.4414765 ,  0.20408715,  0.16630246,  0.37727752, -0.42286447,\n",
       "           0.2003813 ,  0.29182836,  0.10696718,  0.40639636,  0.12139571,\n",
       "          -0.06816009],\n",
       "         [ 0.6175202 , -0.3133241 ,  0.07065044,  0.38512626,  0.4350736 ,\n",
       "          -0.58044064,  0.36954063, -0.3611143 ,  0.35128742, -0.46608782,\n",
       "           0.30637017, -0.6129324 , -0.4390929 ,  0.5532362 ,  0.50107855,\n",
       "          -0.4414214 ],\n",
       "         [ 0.08247249,  0.3706733 , -0.01423504, -0.08671747,  0.2113378 ,\n",
       "           0.3925767 ,  0.08264196, -0.04080154, -0.11568095,  0.36098063,\n",
       "           0.13145131,  0.02301961,  0.21131581, -0.0520365 , -0.20104097,\n",
       "           0.32606065],\n",
       "         [-0.4424588 ,  0.5514444 , -0.39047676, -0.5050668 , -0.7488913 ,\n",
       "           0.33392578, -0.3351058 ,  0.11663508, -0.42831266,  0.6231101 ,\n",
       "          -0.8239557 ,  0.3984965 ,  0.3678983 , -0.28408512, -0.47817007,\n",
       "           0.58087134],\n",
       "         [-0.08712901,  0.09848477, -0.06844285,  0.06989314, -0.2702634 ,\n",
       "           0.21246454, -0.22492695, -0.26223114,  0.11598422,  0.08714783,\n",
       "          -0.04747107,  0.17293498, -0.00427867,  0.10885351,  0.26805413,\n",
       "           0.12614097],\n",
       "         [-0.34034806,  0.4320383 , -0.53727704, -0.37524456, -0.4055827 ,\n",
       "           0.21232125, -0.5329689 ,  0.5704609 , -0.35096413,  0.5988488 ,\n",
       "          -0.44010156,  0.54824185,  0.39113104, -0.37545893, -0.36120537,\n",
       "           0.07459694],\n",
       "         [ 0.78955483, -0.86544025,  0.5775705 ,  0.5486792 ,  0.7710633 ,\n",
       "          -1.1218001 ,  0.394358  , -0.3755787 ,  0.62083524, -0.81730765,\n",
       "           0.8313228 , -0.81498957, -0.6729506 ,  0.5552994 ,  0.6258822 ,\n",
       "          -0.82755154],\n",
       "         [-0.5878507 ,  0.8047273 , -0.50731534, -0.44592452, -0.47411656,\n",
       "           0.89117056, -0.44463322,  0.71821153, -0.45902607,  0.5690076 ,\n",
       "          -0.5815495 ,  0.9224815 ,  0.72162193, -0.5698068 , -0.52612907,\n",
       "           0.11636224],\n",
       "         [ 0.36980966,  0.03104224,  0.18608274,  0.19216321,  0.2700148 ,\n",
       "          -0.19686195,  0.05615221, -0.17250095,  0.25194514, -0.296901  ,\n",
       "           0.3790635 ,  0.08331495,  0.15908375,  0.09738412,  0.23337129,\n",
       "           0.10618926],\n",
       "         [ 0.8204774 , -0.8016767 ,  0.7441736 ,  0.56377316,  0.93382645,\n",
       "          -0.73182803,  0.77290076, -0.46314576,  0.6399284 , -1.054112  ,\n",
       "           0.71816105, -0.65496457, -0.6237543 ,  1.0253655 ,  0.78693634,\n",
       "          -0.6716889 ],\n",
       "         [-0.35053316,  0.48809406, -0.1934817 , -0.22259046, -0.324946  ,\n",
       "           0.06491166, -0.16604853,  0.45649815, -0.03298355,  0.2405696 ,\n",
       "          -0.11904841,  0.21674533,  0.43982318, -0.33949044, -0.2260807 ,\n",
       "           0.0436466 ],\n",
       "         [-0.63132054,  0.40266046, -0.64210916, -0.52661914, -0.6855637 ,\n",
       "           0.80627483, -0.60475993,  0.9105968 , -0.5660815 ,  1.2985382 ,\n",
       "          -0.6199809 ,  0.82597196,  0.93564206, -0.6840973 , -0.6083991 ,\n",
       "           0.45827916],\n",
       "         [ 0.2865426 , -0.5752965 ,  0.31914064,  0.13548358,  0.2598987 ,\n",
       "          -0.48011842,  0.37220043,  0.12243263,  0.19782916, -0.15573244,\n",
       "           0.25489327, -0.06381575, -0.1696247 ,  0.10350625,  0.14090641,\n",
       "          -0.49928102],\n",
       "         [ 0.10639068, -0.42097375,  0.4891159 ,  0.40323627,  0.52981454,\n",
       "          -0.48474926,  0.4382117 , -0.539376  ,  0.49097884, -0.81533957,\n",
       "           0.6285658 , -0.70475763, -0.59243804,  0.51483893,  0.3679806 ,\n",
       "          -0.58003354]], dtype=float32)>,\n",
       "  <tf.Variable 'dense_1/bias:0' shape=(16,) dtype=float32, numpy=\n",
       "  array([ 0.16503192,  0.11247798,  0.18142736,  0.24108437,  0.08486772,\n",
       "          0.10313497,  0.10905385,  0.04442763,  0.1723013 , -0.00911296,\n",
       "          0.21345861,  0.04104795,  0.03314756,  0.2614011 ,  0.26312852,\n",
       "          0.08114803], dtype=float32)>,\n",
       "  <tf.Variable 'dense_2/kernel:0' shape=(16, 1) dtype=float32, numpy=\n",
       "  array([[ 1.1532029 ],\n",
       "         [-0.8566712 ],\n",
       "         [ 1.1401507 ],\n",
       "         [ 1.4197506 ],\n",
       "         [ 0.89635086],\n",
       "         [-1.0338161 ],\n",
       "         [ 1.0734632 ],\n",
       "         [-1.4413311 ],\n",
       "         [ 1.2323172 ],\n",
       "         [-0.74805456],\n",
       "         [ 0.9741496 ],\n",
       "         [-1.0058975 ],\n",
       "         [-1.08413   ],\n",
       "         [ 1.0741873 ],\n",
       "         [ 1.3938535 ],\n",
       "         [-1.2067819 ]], dtype=float32)>,\n",
       "  <tf.Variable 'dense_2/bias:0' shape=(1,) dtype=float32, numpy=array([-0.83987993], dtype=float32)>],\n",
       " 'history': <tensorflow.python.keras.callbacks.History at 0x109807ac8>,\n",
       " 'stop_training': False}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(baseline_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Restore model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - yaml : 마크업언어, 설정파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train,y_train),(X_test,y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_target = y_train[:1000]\n",
    "test_target = y_test[:1000]\n",
    "\n",
    "train_data,test_data = X_train[:1000].reshape(-1,28*28)/255.0, X_test[:1000].reshape(-1,28*28)/255.0\n",
    "#2차원 데이터를 flatten한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_63 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(512, activation='relu', input_shape=(784,)),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "        metrics=['acc']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ModelCheckpoint : callback에서 한번 할때마다 저장된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_callback = tf.keras.callbacks.ModelCheckpoint('training_1/cp.ckpt',\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples\n",
      "Epoch 1/10\n",
      " 864/1000 [========================>.....] - ETA: 0s - loss: 0.0074 - acc: 1.0000\n",
      "Epoch 00001: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 0s 149us/sample - loss: 0.0072 - acc: 1.0000\n",
      "Epoch 2/10\n",
      " 608/1000 [=================>............] - ETA: 0s - loss: 0.0059 - acc: 1.0000\n",
      "Epoch 00002: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 0s 114us/sample - loss: 0.0062 - acc: 1.0000\n",
      "Epoch 3/10\n",
      " 736/1000 [=====================>........] - ETA: 0s - loss: 0.0057 - acc: 1.0000\n",
      "Epoch 00003: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 0s 102us/sample - loss: 0.0056 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " 576/1000 [================>.............] - ETA: 0s - loss: 0.0053 - acc: 1.0000\n",
      "Epoch 00004: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 0s 112us/sample - loss: 0.0051 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " 992/1000 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 1.0000\n",
      "Epoch 00005: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 0s 130us/sample - loss: 0.0048 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " 704/1000 [====================>.........] - ETA: 0s - loss: 0.0043 - acc: 1.0000\n",
      "Epoch 00006: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 0s 101us/sample - loss: 0.0044 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " 704/1000 [====================>.........] - ETA: 0s - loss: 0.0047 - acc: 1.0000\n",
      "Epoch 00007: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 0s 103us/sample - loss: 0.0044 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " 672/1000 [===================>..........] - ETA: 0s - loss: 0.0035 - acc: 1.0000\n",
      "Epoch 00008: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 0s 102us/sample - loss: 0.0036 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " 640/1000 [==================>...........] - ETA: 0s - loss: 0.0035 - acc: 1.0000\n",
      "Epoch 00009: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 0s 122us/sample - loss: 0.0034 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " 448/1000 [============>.................] - ETA: 0s - loss: 0.0030 - acc: 1.0000\n",
      "Epoch 00010: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 0s 120us/sample - loss: 0.0031 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb567b50f0>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, train_target,  epochs = 10, callbacks = [cp_callback]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0xb567e3240>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('training_1/cp.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - 매 에포크마다 다 다른파일로 저장하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# 파일 이름에 에포크 번호를 포함시킵니다(`str.format` 포맷)\n",
    "checkpoint_path = \"training_2/cp-{epoch:04d}.ckpt\" # 매 에포크마다 다 다른 파일로 저장할 수 있다.\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path, verbose=1, save_weights_only=True,\n",
    "    # 다섯 번째 에포크마다 가중치를 저장합니다\n",
    "    period=5)\n",
    "\n",
    "model = create_model()\n",
    "model.save_weights(checkpoint_path.format(epoch=0))\n",
    "model.fit(train_images, train_labels,\n",
    "          epochs = 50, callbacks = [cp_callback],\n",
    "          validation_data = (test_images,test_labels),\n",
    "          verbose=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - 가장 마지막의 에포크를 저장하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "latest\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0xb552fc4a8>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#수동으로 가중치 저장하기\n",
    "model.save_weights('./checkpoints/my_checkpoint')\n",
    "\n",
    "model = create_model()\n",
    "model.load_weights('./checkpoints/my_checkpoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 전체 저장하기\n",
    "     - 모델을 저장해 Tensorflow.js로 불러들일 수 있다.\n",
    "     - PC에서 학습시키는게 가장 빠르다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### HDF5 파일로 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 0s 233us/sample - loss: 1.1045 - acc: 0.6970\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.4060 - acc: 0.8880\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 88us/sample - loss: 0.2780 - acc: 0.9290\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 85us/sample - loss: 0.1939 - acc: 0.9580\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 85us/sample - loss: 0.1547 - acc: 0.9650\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.fit(train_data, train_target, epochs=5)\n",
    "model.save('./my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No dataset in HDF5 file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-167-0603be9e0a6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_hdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./my_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mread_hdf\u001b[0;34m(path_or_buf, key, mode, **kwargs)\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No dataset in HDF5 file.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m             \u001b[0mcandidate_only_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No dataset in HDF5 file."
     ]
    }
   ],
   "source": [
    "hdf = pd.read_hdf('./my_model.h5') # 데이터의 형태가 계층적이기 때문에 pandas의 직사각형 형태와 맞지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 0s 227us/sample - loss: 1.1881 - acc: 0.6340\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 86us/sample - loss: 0.4323 - acc: 0.8750\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 84us/sample - loss: 0.2953 - acc: 0.9220\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 89us/sample - loss: 0.2211 - acc: 0.9450\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 88us/sample - loss: 0.1584 - acc: 0.9640\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "to_json() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-173-6dd0ddf10860>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./my_modeljson.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: to_json() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.fit(train_data, train_target, epochs=5)\n",
    "model.to_json('./my_modeljson.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
